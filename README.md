# ğŸ’¬ Mon Chat Local avec Ollama

Un mini projet pour avoir son **ChatGPT maison** en local/offline.
Backend : [Ollama](https://ollama.com/) pour exÃ©cuter le modÃ¨le.
Frontend : une petite UI en HTML/JS (style ChatGPT sobre).
Proxy : un serveur Node/Express qui redirige vers lâ€™API Ollama.

---

## âš™ï¸ Installation

### 1. Installer Ollama

* TÃ©lÃ©chargez Ollama depuis [ollama.com](https://ollama.com/).
* VÃ©rifiez quâ€™il tourne :

  ```bash
  curl http://localhost:11434/api/tags
  ```

### 2. TÃ©lÃ©charger un modÃ¨le

Liste des models disponible [ollama.com//models](https://ollama.com/search).

Exemple avec Llama 3 :

```bash
ollama pull llama3.2:1b:
```

### 3. Cloner ce repo

```bash
git clone https://github.com/Pvpasall/local-chatgpt.git
cd local-chatgpt
```

### 4. Installer les dÃ©pendances serveur

```bash
npm install
```

### 5. Lancer le proxy Node

```bash
node server.js
```

---

## ğŸš€ Utilisation

1. Ouvrez [http://localhost:3000](http://localhost:3000)
2. Ã‰crivez un message dans lâ€™UI
3. Ollama rÃ©pond via le modÃ¨le choisi (par dÃ©faut `llama3`)

---

## ğŸ“‚ Structure

```
local-chatgpt/
â”œâ”€â”€ server.js       # Proxy Express -> Ollama
â””â”€â”€ public/
    â””â”€â”€ index.html  # UI minimaliste style ChatGPT
```

---

## ğŸ“ Notes

* Tout tourne **localement**, pas de cloud.
* Pour utiliser un autre modÃ¨le : changez `model: 'llama3.2:8b'` dans `server.js` ou directement dans lâ€™UI.
* Pour Ã©viter des erreurs CORS si vous servez lâ€™UI ailleurs :

  ```bash
  export OLLAMA_ORIGINS=http://localhost:3000
  ollama serve
  ```

---

## ğŸ“œ Licence

MIT â€” utilisez, modifiez et partagez librement âœŒï¸
